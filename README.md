# EX-5-prompt-engineering-Comparative Analysis of different types of Prompting patterns and explain with Various Test scenerios

### Aim:
To test how ChatGPT responds to naÃ¯ve prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios, analyzing the quality, accuracy, and depth of the generated responses.

### Instructions:
1.Define the Two Prompt Types:
Naive Prompts: Broad, vague, or open-ended prompts with little specificity.Basic Prompts: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.

2.Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naÃ¯ve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.

3.Run Experiments with ChatGPT:
Input the naÃ¯ve prompt for each scenario and record the generated response.Then input the corresponding basic prompt and capture that response.Repeat this process for all selected scenarios to gather a full set of results.

4.Evaluate Responses :Â 
Compare how ChatGPT performs when given naÃ¯ve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naÃ¯ve prompts work equally well?

Deliverables:
A table comparing ChatGPT's responses to naÃ¯ve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPTâ€™s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.

### 1. Introduction

Large Language Models represent one of the most significant innovations in the field of artificial intelligence in recent years. These models are based on transformer neural architectures, characterized by self-attention mechanisms that allow for the processing of input sequences while maintaining contextual relationships between various elements. Their operation is based on a training process on enormous textual datasets, duringÂ which the model learns to predict the next token in a sequence, optimizing billions of parameters through the backpropagation process. The transformer architecture, introduced in the paper â€œAttention Is All You Needâ€ [1], distinguishes itself through its ability to process input in parallel, overcoming the limitations of traditional recurrent neural networks (RNNs). TheÂ attention mechanism allows the model to weigh differently the importance of various parts of the input, creating dynamic contextual representations. This is made possible through three main components: the query, key, andÂ value matrices, which together allow the model to establish complex correlations between different elements of the input. Large Language Models operate through a tokenization process that converts text into numerical sequences, using vocabularies that can contain tens or hundreds of thousands of tokens. Each token is then embedded in a high-dimensional vector space, where semantic and syntactic relationships are captured through vector distances. This distributed representation allows the model to capture complex linguistic nuances and generalize to cases never seen during training. A distinctive characteristic of modern Large Language Models is their few-shot and zero-shot [2] learning capability, meaning they can adapt to new tasks without the need for further training, simply through examples or textual instructions (prompts). This is made possible by their deep architecture and broad exposure to various contexts during training, allowing them to develop a sort of generalized languageÂ â€œunderstandingâ€.

Large Language Models are increasingly integrated into everyday life, making it crucial to understand and learn the correct interaction methods to effectively interface with these tools. TheÂ countless parameters that define Large Language Models are sufficient to emulate natural conversations. Mastering best practices in prompting is essential, asÂ even minor variations in a prompt can significantly affect the outcome. InÂ light of this, we have undertaken a research study aimed at quantifying the performance of multitask prompts in comparison to single-task prompts. Our initial hypothesis posits that single-task prompts, due to their inherently lower complexity, should yield better results than their multitask counterparts. While this assumption may seem intuitive, quantifying these differences and conducting a comparative analysis across various models is essential for understanding the performance degradation in prompting. Furthermore, despite the intuition that simpler, single-task prompts should produce better results, empirical data suggest that this is not universally true for all Large Language Models. Indeed, while certain models support our initial hypothesis, others exhibit superior performance with multitask prompts. This observation highlights the necessity for a thorough analysis to understand the underlying dynamics of each model. TheÂ differences, particularly those seen in models such as LLama 3.1 and Mistral, demonstrate that the interaction between a prompt and a model cannot be reduced to a simple general rule. This makes our study critical for optimizing the use of Large Language Models in variousÂ pipelines.

The objective of this study is to systematically and quantitatively analyze the performance differences between single-task and multitask prompts across a variety of large-scale language models. TheÂ goal is to provide empirical data that can guide developers and users in selecting the most effective prompting strategy for each model and use case. Our methodology includes defining a series of standardized tasks, which will be presented to various Large Language Models in both single-task and multitask formats. Performance will be measured using a composite metric that combines the results of three metrics applied to specific natural language processing tasks: F1 score for named entity recognition, exact match for sentiment analysis, andÂ Bilingual Evaluation Understudy (BLEU) for review coherence. The findings from this research could have significant practical implications. Firstly, they provide data-driven guidelines to optimize interaction with Large Language Models in various application contexts. Additionally, any discrepancies observed among the models may offer valuable insights into the architectural and training differences that influence how Large Language Models respond to prompt complexity. One of the most well-known challenges of Large Language Models is the interpretability of their output. This study highlights this issue by revealing â€œnon-standardâ€ results when using what would be considered a standardÂ approach.

### 2. RelatedÂ Work

Recent research has explored various prompt engineering techniques for enhancing Large Language Modelsâ€™ (Large Language Models) performance in natural language processing (NLP) tasks. Studies have investigated different types of prompts, including discrete, continuous, few-shot, andÂ zero-shot approaches [3]. Discrete prompts are formulated using natural language, making them interpretable and easier to design, whereas continuous prompts leverage embeddings that are optimized through gradient-based methods, offering more flexibility but requiring specialized tuning techniques. Few-shot and zero-shot prompting techniques allow Large Language Models to perform tasks with minimal or no task-specific training examples, significantly reducing the need for extensive labeled data [3]. These strategies have shown promise in various contexts, highlighting the adaptability of Large Language Models to new tasks with limitedÂ supervision.
Researchers have also developed a catalog of prompt patterns to solve common problems when interacting with Large Language Models [4]. These patterns, which include guidelines for crafting effective prompts, address issues such as prompt ambiguity, model misinterpretations, andÂ response consistency [4]. They provide a taxonomy of prompt patterns that categorize different approaches based on task types, such as classification, generation, andÂ summarization, thus offering a structured framework for prompt design that can be easily applied across different NLP tasks. The effectiveness of different prompting strategies, such as simple prefix, cloze, chain-of-thought, andÂ anticipatory prompts, has been empirically evaluated for clinical NLP tasks [5]. ForÂ instance, theÂ chain-of-thought prompting technique encourages Large Language Models to break down complex tasks into intermediate reasoning steps, improving performance on tasks that require logical progression and detailed explanation. Anticipatory prompts, onÂ the other hand, guide models to predict subsequent responses by leveraging prior knowledge of likely outcomes, enhancing the coherence and relevance of generated text, particularly in domains requiring domain-specific reasoning like clinical NLP [5].

Additionally, novel prompting techniques like heuristic and ensemble prompting have been introduced [5]. Heuristic prompting involves using domain-specific rules or prior knowledge to construct prompts that better align with the task at hand, while ensemble prompting combines multiple prompts to aggregate the strengths of different strategies, enhancing overall model performance and reducing variability in responses. These approaches provide robust alternatives to traditional single-prompt methods, demonstrating the potential of combining multiple prompt types for more reliable outputs. While most studies focus on single-task prompts, some research has explored multitask prompting approaches across various applications, fromÂ questionâ€“answering to commonsense reasoning [6]. Multitask prompting aims to create a single prompt that can handle multiple related tasks, thus improving the efficiency of Large Language Models by reducing the need to design task-specific prompts. This approach has been particularly beneficial in scenarios where models must adapt quickly to a wide range of questions or tasks without retraining, underscoring the versatility of prompt engineering as a technique for broadening the applicability of Large Language Models [6]. These advancements in prompt engineering contribute to improving Large Language Modelsâ€™ performance across diverse NLP tasks without modifying core model parameters. ByÂ optimizing the way models interact with input prompts, researchers are able to enhance Large Language Modelsâ€™ capabilities in handling complex, varied, andÂ specialized tasks, driving forward the field of NLP and expanding the utility of these powerful models in real-worldÂ applications.

The Relation Between Prompt Architecture and Large Language ModelÂ Size
Recent research has delved into the interplay between the size of Large Language Models (LLMs) and the architecture of prompts used to guide them. TheÂ consensus emerging from these studies is that, while larger models generally benefit from prompts containing a greater number of examples, enhancing their overall accuracy and reliability [7], this is not an absolute rule. InÂ particular contexts, smaller, more specialized models have been shown to outperform their larger counterparts. This phenomenon is especially evident in highly specific domains where smaller LLMs, fine-tuned on domain-specific data, demonstrate superior accuracy and efficiency compared to generalized, larger models [8]. These findings underscore the importance of model specialization and the targeted use of data, suggesting that model size alone is not a definitive predictor of performance. Moreover, theÂ structure and complexity of prompts themselves play a critical role in determining LLM performance. Research has demonstrated that simple, straightforward prompt structures are often more effective for knowledge retrieval tasks compared to prompts that employ complex grammatical constructions. ForÂ instance, LinzbachÂ etÂ al. [9] found that prompts with less syntactic complexity allow models to retrieve relevant information more consistently, minimizing the risk of misinterpretation or erroneous outputs. This insight has significant implications for the design of user interactions with LLMs, suggesting that clarity and simplicity should be prioritized when crafting prompts, especially when the goal is to extract factual or technicalÂ information.

However, theÂ sensitivity of LLMs to minor variations in prompt formatting remains a substantial challenge. Even subtle changesâ€”such as the reordering of examples, shifts in tone, orÂ slight alterations in wordingâ€”can have a pronounced impact on performance. InÂ few-shot learning contexts, where LLMs are provided with only a handful of examples to guide their responses, SclarÂ etÂ al. [10] observed discrepancies in accuracy reaching as high as 76 percentage points due to minimal prompt adjustments. Notably, this sensitivity is not confined to any specific model size or level of training; it persists across a wide spectrum of LLMs, independent of the number of examples given or the extent of instruction tuning applied. These findings highlight an inherent instability in LLM responses that complicates their evaluation, making it difficult to establish reliable benchmarks. To mitigate this variability, it has been suggested that assessments of LLMs should not rely on a single-prompt format. Instead, evaluations should encompass a diverse set of prompt configurations to capture a more accurate picture of model performance. Such an approach acknowledges the influence that prompt design can exert on results and seeks to account for this factor in comparative analyses. InÂ response to these challenges, SclarÂ etÂ al. [10] introduced the concept of â€œFormatSpreadâ€, a methodology designed to systematically examine how variations in prompt format impact LLM outputs. FormatSpread aims to provide a more nuanced evaluation framework that includes an array of prompt formats, thereby enabling a more comprehensive understanding of model behavior and capabilities. The implications of these studies are far-reaching, asÂ they call into question some common assumptions about LLM development and evaluation. TheÂ findings suggest that model size, while important, should not overshadow considerations of prompt design, specialization, andÂ sensitivity. Furthermore, they highlight the need for a more sophisticated evaluation paradigm that acknowledges the complexities of language prompts and their effects on model performance. Researchers are increasingly advocating for evaluation protocols that reflect the diverse ways in which LLMs might be engaged in practical applications, thereby ensuring that performance metrics are both robust and representative of real-world usageÂ scenarios.

To further build upon the discussion, our empirical study seeks to highlight the inherent complexity in determining the optimal use of single-task versus multitask prompts within a fixed-prompt architecture. Existing research often contrasts these approaches in specific contexts, suggesting that single-task prompts may offer precision and focus when handling distinct tasks, while multitask prompts provide efficiency and flexibility, especially in environments where models need to generalize across a wide array of related tasks without additional tuning. However, there is no definitive guideline to suggest that one method consistently outperforms the other across all scenarios. Our study aims to empirically demonstrate that the decision between single-task and multitask prompts cannot be universally dictated by a single criterion or strategy. InÂ fixed-prompt architectures, theÂ effectiveness of these approaches is highly dependent on factors such as the complexity of the tasks, theÂ domain specificity, andÂ the nature of the input data. We contend that instead of searching for a one-size-fits-all rule, researchers and practitioners should adopt a context-sensitive strategy, evaluating the trade-offs between precision and generalization based on the specific requirements of the task at hand. This nuanced perspective aligns with the broader trend in prompt engineering research, which emphasizes the importance of tailoring prompts to the unique characteristics of the target application, thus challenging the notion that a universal solution exists for optimizing prompt performance within Large LanguageÂ Models.

### 3. TheÂ Experiment

The objective of our experiment is to generate a JSON output for each observation in the dataset, which will be directly compared to the ground truth JSON. TheÂ dataset holds movies reviews written by users of the web. TheÂ ground truth JSON includes the sentiment label, named entities extracted from the review, andÂ the original review text extracted from the IMDB dataset (as shown inÂ Listing A1). ToÂ achieve this, we employed two distinct approaches: a single-task approach and a multitask approach, as shown in theÂ Figure 1, below. These methodologies were designed to test the efficiency and accuracy of different prompt strategies for Large Language Models in handling multiple tasks simultaneously versusÂ individually.

![image](https://github.com/user-attachments/assets/1fac3eec-ed26-4cf2-96b8-7361f95bc732)

Figure 1.Â Flow chart that describes the experiment workflow.

The two experimental workflows provide a comprehensive framework for evaluating the efficiency and precision of single-task versus multitask prompts. TheÂ outputs from both approaches were systematically compared against the ground truth JSON to determine the following key metrics: accuracy, which involves the correctness of sentiment classification and named entity recognition in both single-task and multitask scenarios; and consistency, which refers to the consistency of JSON formatting and structure across different promptÂ strategies.

### 3.1. TheÂ Dataset
In this study, we constructed our dataset using the IMDB review dataset available on Kaggle [11], aÂ well-established dataset commonly employed in sentiment analysis research. TheÂ IMDB dataset contains movie reviews, each labeled with binary sentiment values indicating whether the sentiment is positive or negative. This dataset forms the basis of our comparative analysis, which focuses on evaluating the performance of prompt strategies for Large Language Models (LLMs) in both sentiment analysis and named entity recognition (NER) tasks. Given the large size of the original IMDB dataset, we performed a random sampling to select 1000 reviews, ensuring a manageable yet representative subset for our experiments. To enrich the dataset with named entity information, we utilized the SpaCy library, aÂ leading tool in NLP for English language tasks [12]. We selected SpaCyâ€™s transformer-based model, en_core_web_trf, known for its ability to capture nuanced contextual information and accurately identify entities. ForÂ each review, we applied SpaCyâ€™s NER function to extract named entities, specifically targeting categories such as persons (PER), locations (LOC), andÂ organizations (ORG). This extraction process involved tokenizing the review text, identifying potential entities using the transformer architecture, andÂ classifying them into relevant categories. Following NER processing, we augmented the dataset by introducing an additional column labeled â€œentitiesâ€. This column contains a structured list of the extracted entities for each review, organized in a dictionary format with the entity type (e.g., PER, LOC, ORG) and the corresponding text identified by the SpaCy model. This structured representation facilitates further analysis and allows us to incorporate entity information seamlessly into our experimentalÂ framework.

The final dataset is composed of three primary components: the original review text, theÂ sentiment label provided by the IMDB dataset, andÂ the named entities extracted through SpaCy. To better clarify the dataset structure, we show an example in theÂ Table 1Â below. We organized this information into a JSON format, where each review is represented by three keys: â€œreviewâ€ for the original text, â€œsentimentâ€ for the sentiment label, andÂ â€œentitiesâ€ for the list of named entities. This JSON structure supports a comprehensive examination of LLM performance across both single-task and multitask prompts, ensuring a rigorous and systematic evaluation of the modelsâ€™ accuracy and effectiveness. By leveraging the IMDB datasetâ€™s extensive sentiment annotations and supplementing it with high-quality named entity data, we created a multifaceted testing ground to assess the efficacy of different prompting strategies for LLMs. This dual-task setup not only provides a clear benchmark for comparative performance but also highlights the practical applications of LLMs in real-world scenarios that require the integration of multiple NLPÂ tasks.

Table 1.Â A single row of the dataset (review text has been truncated to enhance readability).

![image](https://github.com/user-attachments/assets/8e97cc95-1814-4070-8a6f-8e371042ed5e)

Justification for IMDB DatasetÂ Selection:

Our choice to employ the IMDB dataset is grounded in its suitability for studying the capabilities of LLMs in realistic and diverse linguistic environments. TheÂ dataset is composed of user-generated movie reviews, which inherently vary in writing style, linguistic complexity, andÂ formal accuracy. This variety introduces challenges that are essential for evaluating the robustness of LLMs, asÂ user-generated content often includes informal expressions, grammatical errors, andÂ unconventional syntactic structuresâ€”features that are less prevalent in professionally curated texts. The IMDB datasetâ€™s broad adoption in the NLP research community, particularly in sentiment analysis studies, provides a well-defined benchmark for experimental comparison. Its widespread use enables us to position our findings within the context of the existing literature, facilitating a reliable assessment of the efficacy of different prompting strategies. Furthermore, theÂ datasetâ€™s focus on movie reviewsâ€”texts that often combine objective descriptions with subjective opinionsâ€”makes it an excellent resource for tasks requiring both sentiment analysis and NER. By selecting a dataset that closely mirrors the types of language encountered in everyday user interactions, we aim to ensure the relevance and applicability of our study. This choice allows us to evaluate the performance of LLMs in handling complex, real-world linguistic inputs, contributing to a deeper understanding of how these models manage the intricacies of unstructured and diverseÂ language.

### 3.2. The PromptÂ Architecture

The experiment workflow is meant to compare the single-task prompt approach with the multitask prompt approach. We employed a fixed-prompt architecture with the aim to provide a sort of model-agnostic usage. The use of a fixed-prompt architecture in prompt engineering is driven by several factors that enhance the rigor and efficiency of comparative analysis. AÂ fixed set of prompts allows for a fair comparison across different models, asÂ it maintains a consistent input structure. This consistency ensures that the effects observed are due to the modelsâ€™ architecture and capabilities, rather than variations in prompt formulation. InÂ doing so, potential biases from tailored prompts for individual tasks or models are avoided, leading to a more accurate attribution of performance differences. Another advantage of the fixed-prompt approach is its ability to reduce both complexity and computational costs. ByÂ removing the need to optimize prompts for each specific task, theÂ development and testing process becomes more straightforward. This not only saves time but also conserves computational resources, asÂ prompt tuning is often an iterative and resource-intensive endeavor. Furthermore, aÂ fixed-prompt strategy facilitates the assessment of a modelâ€™s generalization and robustness. Using a standardized set of prompts across tasks allows for a clear evaluation of how well a model generalizes, making comparisons between multitask and single-task approaches under uniform conditions possible. This serves as a robust test of a modelâ€™s ability to handle generic, non-optimized inputs. Scalability and ease of maintenance are also supported by this architecture. AÂ uniform set of prompts makes system updates or modifications less complex, since changes do not require re-optimizing prompts for individual tasks. This scalability is particularly advantageous in large-scale applications, where the number of tasks may increase over time, allowing systems to expand without significant increases in maintenance overhead. The choice of a fixed-prompt architecture also enhances reproducibility, which is crucial for academic research. AÂ consistent input context increases the likelihood that results are replicable, providing a stable baseline for evaluating the benefits of adaptive or optimized prompts. This stability allows for a quantitative assessment of prompt optimization by comparing results to a standardized referenceÂ point.

The implications of adopting a fixed-prompt architecture extend to both research and practical applications. InÂ research, it enables more reliable benchmarking across different models, making comparisons more meaningful. Researchers can identify genuine performance differences attributable to the models themselves, rather than adjustments in prompt design, leading to a clearer understanding of the strengths and limitations of various architectures, particularly in multitask versus single-task settings. This approach also places a strong emphasis on generalization. Evaluating models under a consistent set of prompts highlights their ability to handle diverse tasks, pushing the field towards the development of architectures that perform well across a wide range of contexts without extensive adaptation. This could encourage the creation of more versatile and generalizable AI systems, asÂ opposed to those relying on domain-specific prompt optimization. In practical applications, theÂ fixed-prompt strategy significantly impacts scalability. InÂ scenarios where the number of tasks or domains grows over time, using a single set of prompts allows systems to expand efficiently, making large-scale deployment more feasible and cost-effective. This is particularly valuable in commercial settings, where operational efficiency and scalability are key considerations. From a research methodology perspective, aÂ fixed-prompt architecture establishes a solid baseline, enhancing the reproducibility of studies by providing a clear and invariant prompt set. This promotes rigorous scientific discourse, allowing new methods to be evaluated against well-defined standards. However, this approach places significant importance on the initial prompt selection. TheÂ chosen prompts must be representative and carefully designed to ensure fair assessment across a variety of tasks. IfÂ the prompts are poorly selected, they might either obscure a modelâ€™s strengths or exaggerate its weaknesses, potentially leading to misleadingÂ conclusions.

### 3.2.1. The Single-TaskÂ Approach

In the single-task prompt approach, we decoupled the tasks of sentiment classification, named entity recognition (NER), andÂ JSON formatting into separate, distinct operations. TheÂ goal was to isolate each task to determine how effectively Large Language Models can perform when given a dedicated prompt for each task. The process began with sentiment classification. We first used a single-task prompt to classify the binary sentiment of each review. TheÂ Large Language Models were prompted to read the review text and determine whether the sentiment was positive or negative. All 1000 elements in our dataset were processed independently, withÂ each review being fed into the LLM, andÂ a sentiment label (either â€œpositiveâ€ or â€œnegativeâ€) was generated for each review. This operation produced an initial output consisting solely of sentiment classifications. Following sentiment classification, aÂ separate prompt was employed to extract named entities from the reviews. TheÂ entities included people (PER), locations (LOC), andÂ organizations (ORG). Each review was again processed individually through the Large Language Models, this time with a focus on identifying and classifying named entities. TheÂ output of this step was a collection of lists containing the extracted entities for each review. The final step involved formatting the results into a structured JSON format using the outputs from the first two tasks. TheÂ sentiment labels and named entities were combined with the original review text to create a JSON object for each review. Each JSON object included keys for â€œreviewâ€, â€œsentimentâ€, and â€œentitiesâ€, matching the structure of the ground truth JSON. This step ensured that the outputs were aligned with the expected format for comparison. AÂ clarifying UML activity diagram of the approachÂ is displayed below inÂ Figure 2.

![image](https://github.com/user-attachments/assets/d1974aa1-7469-4816-93e6-a87eaa60df5f)

Figure 2.Â UML activity diagram representing the single-task promptsâ€™ execution workflow.
The used single-task prompts are shown inÂ Appendix Listing A3. Single-task prompts were designed to create a chain of invocations where the outputs of two calls are used as input context for the final prompt. TheÂ first two prompts performed named entity recognition (NER) and sentiment analysis classification tasks. TheÂ output of these two tasks then merged and was used as input for the third prompt, whose goal is to reorganize the obtained information, producing a comprehensive JSONÂ output.

### 3.2.2. The MultitaskÂ Approach

The multitask prompt approach was designed to evaluate the performance of Large Language Models when tasked with performing multiple tasks simultaneously. InÂ this method, aÂ single prompt was used to instruct the Large Language Models to carry out sentiment classification, named entity recognition, andÂ JSON formatting in one unified operation. TheÂ detailed prompt is shown inÂ Appendix Listing A2.

In this approach, we used a unified prompting strategy where the Large Language Models were provided with a single, comprehensive prompt for each review. To better clarify the workflow, we provide an UML activity diagram shown inÂ Figure 3. This prompt instructed them to analyze the sentiment, extract named entities, andÂ format the results into a JSON object. All 1000 reviews were processed in a batch manner, withÂ each review being fed into the LLM with a multitask prompt designed to handle all three tasks at once. TheÂ output for each review was a complete JSON object containing the sentiment classification, extracted entities, andÂ the review text itself. By handling sentiment analysis, named entity recognition, andÂ JSON formatting in parallel, theÂ multitask approach leverages the Large Language Modelsâ€™ ability to process complex, integrated prompts. TheÂ final output for each review was a JSON object directly generated by the LLM, structured similarly to the ground truth JSON. This direct approach allows for the assessment of the Large Language Modelsâ€™ capability to multitask effectively andÂ efficiently.

![image](https://github.com/user-attachments/assets/409a02fa-65a9-4a2f-bcc7-137d4e204f1a)

Figure 3.Â UML activity diagram representing the multitask promptâ€™s execution workflow.

### 3.3. ExecutionÂ Environment

The experiment was executed in Jupyter notebooks, which are publicly available on our GitHub repository [13]. This decision aligns with our commitment to transparency and reproducibility, allowing other researchers to access and replicate our findings with ease. TheÂ Jupyter notebooks provide a detailed step-by-step account of the entire experimental workflow, including data preprocessing, prompt formulation, andÂ LLM interactions. By using Jupyter notebooks, we ensure that each experiment is documented in a manner that captures the nuances of our methodology, fromÂ data ingestion to output generation. These notebooks not only contain the code used for executing each task but also include commentary and insights into the decisions made throughout the experiment. This transparency is crucial for fostering collaboration and innovation within the research community. In addition to the code, theÂ dataset utilized in our study is also hosted on the same GitHub repository. TheÂ dataset includes the processed IMDB reviews, along with the generated JSON outputs for both single-task and multitask approaches. ByÂ providing both the data and the code, we facilitate an open-access environment where researchers can easily validate and build upon our work. Our GitHub repository serves as a comprehensive resource for those interested in exploring the intricacies of LLM prompt strategies. It invites further exploration and experimentation, offering a platform for continuous improvement and shared learning within the field of natural languageÂ processing.

TheÂ Temperature

In our study, we deliberately set a fixed temperature of 0.8 for all models. This decision was grounded in the need to maintain a balance between precision and creativity in the outputs. AÂ temperature setting of 0.8 was chosen because it avoids the extremes of determinism and randomness. Lower temperatures, closer to zero, tend to produce highly deterministic responses, reducing variability and creativity, which may inhibit the modelâ€™s ability to explore diverse but relevant outputs. Conversely, higher temperatures might lead to overly creative or tangential responses, increasing the likelihood of generating irrelevant or imaginative content that could obscure the evaluation of the modelsâ€™ true capabilities. By fixing the temperature at this intermediate level, our goal was to create conditions that would allow the models to demonstrate their generalization abilities more effectively. TheÂ selected temperature encourages the generation of outputs that are varied enough to showcase the modelsâ€™ adaptability across tasks, yet sufficiently constrained to remain grounded in relevant contexts. This configuration emphasizes the modelsâ€™ inherent capacity to manage diverse scenarios without relying on extreme prompt tuning or specializedÂ adjustments.
The decision to keep a consistent temperature across all experiments ensures a fair and controlled environment for comparative analysis. It eliminates the variability that might arise from fluctuating temperature settings, thereby allowing us to attribute differences in performance directly to the modelsâ€™ architectures and underlying mechanisms, rather than to variations in sampling strategies. This standardized approach not only enhances the reliability of our findings but also facilitates a deeper exploration of each modelâ€™s potential to generalize effectively across multiple tasks without excessive prompting or guidance. Thus, theÂ fixed temperature of 0.8 was not merely a technical choice, butÂ a strategic decision aimed at fostering a thorough assessment of each modelâ€™s robustness and flexibility, while maintaining a consistent baseline that allows for equitable comparisons. This enabled us to investigate how different architectures handle the inherent complexity and diversity of the prompts, offering insights into their adaptability and generalization capabilities under a balanced but challengingÂ setting.

### 3.4. The Employed Large LanguageÂ Models

In this study, we focused on evaluating the performance of five different open Large Language Models (Large Language Models) for sentiment analysis and named entity recognitionÂ tasks.
The term â€œopenâ€ when referring to Large Language Models, asÂ highlighted by Liesenfeld and Dingemanse [14], encompasses significant differences across various levels of â€œopennessâ€ in AI models, which can be 

categorized as follows:

Open-source: The modelâ€™s source code is fully accessible and modifiable.
Open-weights: The trained model weights are available, butÂ possibly with licensing restrictions.
Open-access: The model can be accessed via API, butÂ without access to the code or weights.
Open-science: The research methodology and results are publicly documented.
In the context of our study, theÂ models used are primarily classified as â€œopen-weightsâ€, asÂ their weights are publicly available and redistributable, thoughÂ with some restrictions specified in their licenses. It is important to note that this level of openness, while significant for research reproducibility, does not necessarily equate to full freedom of use and modification, asÂ would be found in a fully open-source project. Furthermore, asÂ Bielefeld and Dingemanse emphasized, theÂ simple label of â€œopenâ€ can obscure various degrees of restrictions and limitations, which are crucial to understand for an accurate assessment of accessibility and reproducibility in AIÂ research.
The primary motivation behind selecting open-weight models was to enhance the reproducibility of our research. ByÂ choosing models that are accessible to the public, we ensure that other researchers can replicate our experiments, verify our findings, andÂ build upon our work without the constraints often associated with proprietary models. While state-of-the-art Large Language Models such as OpenAIâ€™s GPT, Anthropicâ€™s Claude, and Googleâ€™s Gemini have demonstrated remarkable performance across a wide range of language tasks, their proprietary nature poses challenges for academic research in terms of accessibility and transparency. These models often come with usage restrictions, limited customization options, andÂ require considerable computational resources, which can hinder reproducibility and broader scientific exploration. Therefore, toÂ foster an open and collaborative research environment, we selected a set of high-performing open-weight models that provide a balance between accessibility and capability. The specific 

Large Language Models we employed in our study are as follows:

1.LLama 3.1 8B (8b-instruct-q4_0).
2.Phi3 Medium (14b-medium-128k-instruct-q4_0).
3.Qwen2 7B (7b-instruct-q4_0).
4.Gemma2 9B (9b-instruct-q4_0).
5.Mistral 7B (7b-instruct-v0.3-q4_0).

### 3.4.1. Justification for Employed Large LanguageÂ Models

The selection of these specific open-weight models was guided by several factors. Using open-weighted models, we enhanced the reproducibility of our research, enabling other researchers to replicate our experiments and validate our findings without proprietary restrictions. Open-weight models are generally more accessible, allowing a wider range of researchers and practitioners to engage with the research, regardless of their institutional or financial resources. Although not in absolute state of the art, theÂ chosen models still offer competitive performance in sentiment analysis and named entity recognition tasks, providing meaningful insight into LLM capabilities. Open-weight models benefit from active community participation, which leads to continuous improvements and innovations. This collaborative environment fosters the development of robust models that evolve in response to community needs and feedback. The selected models have varying parameter sizes that allow for experimentation on different computational platforms, fromÂ local machines to more extensive cloud-based infrastructures, facilitating scalable research approaches. In order to determine the five candidates, we observed the â€œOpen LLM Leaderboard 2â€ hosted on Huggingface that lists the open-weight LLM performances [15,16,17,18,19,20,21,22,23]. We accessed this leaderboard in June 2024. ByÂ focusing on open-weight Large Language Models, this study not only provides valuable insights into the effectiveness of prompt strategies, butÂ also contributes to a body of work that prioritizes transparency and accessibility in AI research. This approach aligns with the broader goals of promoting open science and collaborative innovation within the machine learningÂ community.
In this study, theÂ decision to utilize a smaller model, rather than opting for larger and potentially more capable alternatives, was driven by several key considerations. Firstly, smaller models offer substantial advantages in terms of resource efficiency. They require less computational power, memory, andÂ storage, leading to reduced costs and time investment during the experimental phase. This is particularly pertinent when a high volume of experiments must be conducted or when computational resources are constrained. Additionally, theÂ complexity of the tasks under investigation, which focused on single-classification operations, did not necessitate the deployment of the most sophisticated or extensive models. TheÂ relative simplicity of single-classification tasks can often be effectively managed by smaller models without significant losses in accuracy, thereby making their use more practical. Furthermore, theÂ primary objective of this study was to explore the impact of different prompt strategies, specifically comparing single-task prompts with multitask prompts, rather than to assess the raw capabilities of large-scale models. Employing smaller models allowed for the isolation of prompt strategy effects, minimizing the potential influence of a modelâ€™s inherent complexity on the observedÂ outcomes.
The studyâ€™s exclusive focus on single-classification tasks was similarly motivated by the need for a controlled and precise evaluation framework. Single-task classification offers a streamlined environment for analyzing how prompt modifications influence model performance, eliminating the complexities and additional variables that multitask scenarios would introduce. This simplicity enhances the reproducibility and comparability of results across various models and prompt strategies, allowing for clearer and more reliable insights. Moreover, single-classification tasks often serve as foundational elements within more complex natural language processing tasks. ByÂ initially concentrating on these simpler operations, theÂ study establishes a robust baseline for understanding the dynamics of prompt strategies, which could potentially be extended to more intricate multitasking assessments in future research. InÂ sum, theÂ decision to employ smaller models and to focus on single-task classification facilitated a research environment in which the effects of prompt strategies could be isolated and scrutinized with greaterÂ clarity.

### 3.4.2. The Adoption ofÂ Ollama

Ollama is an open-source framework developed to facilitate the local deployment and execution of Large Language Models (LLMs). InÂ our experiments, Ollama played a pivotal role, serving as the platform for deploying the five chosen LLMs: LLama 3.1 7B, Phi3 Medium, Qwen2 7B, Gemma2 7B, andÂ Mistral 7B. Its selection was driven by its user-friendly interface, ease of use, andÂ strong alignment with our commitment to reproducibility, which is central to our research objectives. ByÂ employing Ollama, we ensured that our experimental setup remains accessible and easily replicable, fostering transparency and collaboration within the researchÂ community.
Ollamaâ€™s architecture is built upon a REST API, which simplifies model interactions through automated management processes. TheÂ framework automatically checks the local availability of requested models; if a model is not found, Ollama alerts the user and facilitates a direct download. AfterÂ acquisition, theÂ framework autonomously manages the configuration process, optimizing the model according to the available hardware. These steps encompass the extraction of model files, theÂ fine-tuning of memory resources through quantization, andÂ the balancing of computational loads across the RAM, CPU, andÂ GPU. Additionally, Ollama integrates robust error management, verifying the integrity of downloaded files and recording any discrepancies to aid debugging. This architectural design ensures compatibility across major operating systems, making Ollama a versatile and accessible choice for both researchers and practitioners. InÂ our study, this cross-platform compatibility was crucial, asÂ it enabled a consistent and reliable setup regardless of the specific hardware used by different members of the researchÂ community.
A core feature of Ollama is its implementation of quantization, aÂ technique that significantly reduces the numerical precision of model parameters while maintaining acceptable levels of accuracy. This reduction involves converting high-precision data types like float32 (32-bit) to more compact formats such as int8 (8-bit) or int4 (4-bit). Quantization drastically lowers memory consumption, aÂ vital advantage for the local execution of large models. ForÂ example, aÂ seven billion parameter model in float32 format may require up to 28 GB of RAM, whereas the int8 version requires only about 7 GB. This efficiency was particularly advantageous in our experiments, asÂ it allowed us to run multiple high-parameter models locally without compromising performance, facilitating efficient and reproducibleÂ testing.
Ollama utilizes a standardized inference pipeline, structured into four primary phases, toÂ optimize the execution of LLMs as shown inÂ Figure 4. This structured approach was instrumental in our study, ensuring consistent and high-quality output across various models. The first phase, preprocessing, involves the preparation of input data. Text is tokenized into discrete units that the model can process, aÂ context is constructed to guide comprehension, andÂ memory resources are dynamically allocated. This ensures that each model receives input in a format conducive to accurate processing. In the execution phase, theÂ framework loads the quantized model into memory and sequentially processes tokens. Effective cache management during this phase minimizes latency by reusing previously computed results, optimizing repeated tasks. The generation phase is dedicated to transforming processed tokens back into human-readable text. Advanced sampling techniques, such as beam search and nucleus sampling, are employed to enhance the quality of generated output, while incremental generation enables real-time responses. This step-by-step text generation was particularly useful in our research, asÂ it allowed us to monitor model outputs in real time, facilitating adjustments where necessary. Finally, theÂ postprocessing phase involves refining the generated output. Text is formatted into a coherent structure, memory resources are released, andÂ the final output is prepared in JSON format for exposure via the API. This systematic approach, coupled with Ollamaâ€™s user-friendly interface, allowed us to maintain a reproducible and transparent workflow throughout ourÂ experiments.

![image](https://github.com/user-attachments/assets/5f603fba-913c-4241-9fbf-016494f907d8)

Figure 4.Â Flow chart that describes the Ollama inference pipeline.

Our choice of Ollama was not merely technical but also philosophical. TheÂ platformâ€™s straightforward installation and configuration allowed us to maintain a high level of accessibility, ensuring that others can easily replicate our setup without extensive technical expertise. Its active support for a broad range of open-weight models, including those integral to our research, was another decisive factor. ByÂ enabling a seamless integration of diverse LLMs, Ollama ensured consistent execution across different experimental scenarios, contributing to the robustness of our findings. By leveraging Ollama, we adhered to our commitment to open science. TheÂ platformâ€™s widespread usability means that any researcher interested in our work can replicate our environment and methodology with minimal effort. This accessibility not only bolsters the credibility of our results but also encourages further validation and exploration, thereby fostering a collaborative research ecosystem. Our decision to use Ollama aligns with a broader movement towards transparent and reproducible science, contributing to a culture of open inquiry and sharedÂ discovery.

### 3.5. Evaluation

In order to rigorously assess the performance of the Large Language Models (Large Language Models) employed in our study, we designed a comprehensive evaluation metric that integrates three critical components: sentiment exact match, named entity recognition (NER) performance, andÂ review text fidelity. Our evaluation metric is designed to provide a holistic measure of how effectively the Large Language Models handle the tasks of sentiment classification, named entity extraction, andÂ text reproduction. The evaluation metric is defined as follows:
(ğ‘¥,ğ‘¦,ğ‘§)=ğ‘¥+ğ‘¦+ğ‘§3f(x,y,z)=x+y+z3
whereÂ xÂ is the exact match of sentiment detection, defined asÂ follows:
ğ‘¥={10ifthedetectedsentimentiscorrectotherwisex=1ifthedetectedsentimentiscorrect0otherwise
Then,Â yÂ represents the F1 score of the named entity recognition (NER). True positives are entities present in both the ground truth and the output. False negatives refer to those missing from the output but included in the ground truth, while false positives are found in the output but absent from the groundÂ truth.
This score reflects the precision and recall balance achieved by the Large Language Models in identifying and classifying named entities within the review text. TheÂ â€œexact matchâ€ approach has been used here.Â zÂ is the BLEU (Bilingual Evaluation Understudy) score of the review, which measures how closely the generated review matches the original text. This component of the metric evaluates the LLMâ€™s ability to reproduce the review text accurately. The functionÂ (ğ‘¥,ğ‘¦,ğ‘§)f(x,y,z)Â computes the arithmetic mean ofÂ x,Â y, andÂ z, providing an overall performance score for each review processed by the Large Language Models. ByÂ averaging these three metrics, we achieve a balanced evaluation that considers both classification accuracy and text generation quality. The rationale behind this metric is to ensure that each aspect of the task is equally weighted, acknowledging the importance of sentiment exact match, entity recognition precision, andÂ fidelity to the original review text. This holistic approach allows us to capture the multifaceted nature of the task, offering a robust framework for evaluating the effectiveness of different prompting strategies in Large LanguageÂ Models.
The LLMâ€™s output is postprocessed in order to extract the right JSON data. This is due to the fact that Large Language Models often respond discoursively without providing strict JSON. Two distincts regular expressions are used in this scenario. TheÂ former one, mentioned inÂ Appendix Listing A4Â is used to remove JSON comments. Sometimes, Large Language Models follow the JSONC format for their answers instead of JSON, so we need to remove all the comments in order to parse the data correctly. Then, theÂ second one, mentioned inÂ Appendix Listing A5, is a regular expression used to extract the JSON string. Ultimately, it extracts the group starting from the first left curly bracket and ending with the last right curly bracket. Moreover, Large Language Models tend to respond with a JSON that uses single apices instead of double quotes. So, inÂ a pythonic manner, the algorithm works as shown inÂ Appendix Listing A6, assuming that the two regular expressions have already been applied. When the parsing fails, we attribute a score of 0% to thatÂ case.

### 4. Results

The analysis of our experimental results provides a nuanced perspective on the efficacy of multitask versus atomic single-task prompts. Contrary to our initial hypothesis, theÂ data reveal that the atomic single-task prompt approach does not uniformly outperform a multitask prompt across all contexts. Our study highlights significant variability in prompt effectiveness depending on the specific model used. This observation suggests that the interaction between prompt type and model architecture is complex and warrants careful consideration. Specifically, theÂ performance of a given prompt can be highly sensitive to the underlying modelâ€™s characteristics, indicating that model-specific factors play a crucial role in determining the relative success of prompting strategies. In detail, our experiments yielded mixed outcomes. Out of the five distinct experimental setups, three demonstrated that atomic single-task prompts were more effective than their multitask counterparts. These results suggest that for certain tasks, simpler and more specialized prompts may offer advantages in terms of accuracy or efficiency. Conversely, two experiments showed that multitask prompts provided superior performance, challenging the assumption that simplicity inherently leads to better outcomes. This variability underscores the importance of tailoring the prompting approach to the specific task and model, rather than relying on a one-size-fits-allÂ strategy.
Furthermore, theÂ unexpected nature of our findings is worth noting. Despite the theoretical benefits of atomic single-task prompts such as the potential for improved efficiency and generalization, the empirical evidence from our study does not consistently support these advantages. We had anticipated that the low complexity associated with single-task prompts would correlate with enhanced performance. However, theÂ results indicate that this expectation does not always hold true in practice. TheÂ complexity of single-task prompts did not translate into universally superior outcomes compared to the relatively straightforward multitask prompts. Additionally, our investigation included a range of models with varying sizes, fromÂ 2 billion to 14 billion parameters. TheÂ results from these experiments did not reveal a clear relationship between model size and prompt effectiveness. This finding suggests that the performance of prompting strategies is not solely dependent on the scale of the model but is influenced by other factors, such as task characteristics and prompt design. TheÂ Table 2Â below shows the mean scores for each model and approach, where â€œscoresâ€ mean the metric explained inÂ Section 3.
Table 2.Â Performance scores of different models with single-task and multitask approaches acrossÂ metrics.

![image](https://github.com/user-attachments/assets/889b4c9d-280f-41fb-8c8f-798bb6f44e04)


### 4.1. StasticalÂ Significance
In this section, we present a series of statistical tests conducted to provide additional evidence supporting our findings. First, we performed the Shapiroâ€“Wilk test to assess whether the distributions of our data were normal. TheÂ results clearly indicate that all distributions deviate significantly from normality, asÂ reflected by the extremely lowÂ p-values reported inÂ Table 3. TheseÂ p-values, consistently below conventional thresholds (e.g., 0.05), strongly suggest that none of the distributions for the evaluated metrics follow a normalÂ pattern.

Table 3.Â Shapiroâ€“Wilk testÂ p-values were measured for each model to test if the distributions areÂ normal.

![image](https://github.com/user-attachments/assets/297e99ba-5748-4609-b30f-c851049f9695)


Given the non-normal nature of the data, we employed the Wilcoxon signed-rank test for continuous metrics such as F1, BLEU, andÂ the overall score. ForÂ the categorical metric, specifically the Exact-Match score for Sentiment (which can take a binary value of 0 or 1 depending on whether the output matches the ground truth), we opted for the McNemar test. TheÂ decision to use these non-parametric tests ensures that our analysis remains statistically valid despite the deviations from normality. TheÂ results are summarized inÂ Table 4.

Table 4.Â TheÂ p-values for different models using various evaluationÂ metrics.

![image](https://github.com/user-attachments/assets/0a141b36-7f1d-4adb-9520-a7ec01a0860f)

Expanding upon the results, theÂ findings provide valuable insights into how different models react to single-task versus multitask prompting strategies across a variety of evaluation metrics. Each metric presents a distinct pattern of sensitivity, indicating that the effectiveness of the prompting approaches is not uniform across tasks or models, highlighting the nuanced behavior of Large LanguageÂ Models.
For the NER F1 metric, theÂ Wilcoxon test identified significant differences in most models, suggesting that the choice between single-task and multitask prompts can substantially influence the modelâ€™s ability to recognize named entities. However, theÂ exception of LLama 3.1 8B, withÂ aÂ p-value just above the conventional threshold of 0.05, indicates a borderline case where the prompting approach may have an impact, butÂ not to a statistically significant extent. This borderline result suggests that LLama 3.1 8B might be relatively resilient to changes in prompt structure for NER tasks, orÂ that its response to such changes lies within a range of performance variability where differences do not reach statistical significance. This observation could warrant further investigation, possibly involving larger sample sizes or additional metrics, toÂ clarify if this lack of significance is inherent to the model or a result of sample-specificÂ variability.In the case of the BLEU score, which is a critical measure for assessing the quality of generated text in review scenarios, theÂ Wilcoxon test demonstrated overwhelmingly significant results for most models, withÂ Phi3 Medium and Qwen 2 7B exhibiting particularly strong effects. TheÂ extremely lowÂ p-values suggest that these models are highly sensitive to the structure of the prompt when tasked with generating coherent and accurate text. This sensitivity implies that for text generation tasks, selecting the appropriate prompting strategy is crucial, asÂ it can lead to substantial differences in the quality of the generated output. TheÂ stark significance across multiple models also suggests that multitask prompting could provide a structured context that enhances generative quality, particularly for complex or multi-dimensional output likeÂ reviews.The results for the Sentiment Score, analyzed using the McNemar test, present a more heterogeneous picture. Only Mistral 7B exhibited a strong statistically significant result, suggesting that it is particularly responsive to the choice of the prompting strategy when it comes to binary sentiment classification. InÂ contrast, theÂ other models showedÂ p-values around or above the 0.05 threshold, indicating a weaker or non-significant differentiation between single-task and multitask prompts in sentiment tasks. This outcome implies that for many models, sentiment classification may be relatively stable across prompting strategies, orÂ that the gains from multitask prompting are not as pronounced in this binary classification context. It is possible that the simplicity of binary sentiment decisions, compared to more nuanced tasks like named entity recognition or text generation, reduces the sensitivity to prompt changes, particularly in models that are already well tuned for such binary classificationÂ tasks.Lastly, theÂ overall Score metric, which synthesizes multiple aspects of performance, reinforces the impact of prompting strategies, asÂ indicated by the statistically significant differences observed for the majority of models. These results suggest that across a broad spectrum of metrics, theÂ choice of prompt affects model performance, underscoring the importance of prompt engineering in optimizing LLM outputs. However, theÂ lack of significance for Mistral 7B, asÂ demonstrated by aÂ p-value of 0.60, suggests a robustness in its performance, regardless of the prompting strategy. This consistency may indicate that Mistral 7B has a more uniform handling of information across different prompts, potentially due to architectural factors or training data characteristics that enable it to maintain stable performance. It could also reflect an inherent stability in how Mistral 7B processes diverse inputs, making it less prone to fluctuations in performance based on promptÂ structuring.

### Statistical Tests to Compare Large LanguageÂ Models

To compare the performance of the various models across different metrics and prompting conditions, we employed the Friedman test, aÂ non-parametric statistical test suitable for analyzing data of repeated measures. This choice was motivated by the need to assess multiple models under the same conditions while avoiding assumptions of normality, which do not hold for our data distributions. TheÂ Friedman test allowed us to determine whether there were statistically significant differences between the modelsâ€™ performances in both single-task and multitask scenarios, providing a robust comparison framework that accounts for the inherent dependencies in our experimentalÂ design.Based on the Friedman test results shown inÂ Table 5, it is clear that significant differences exist between the performance of the evaluated models across all metrics, underÂ both single-task and multitask prompting conditions. Each metric consistently demonstratesÂ p-values that are orders of magnitude below typical thresholds for statistical significance (e.g., 0.05), indicating that the models exhibit distinct behaviors depending on the prompting strategy used. This consistent pattern of significance underscores the critical role that prompt structuring plays in shaping the output quality of Large LanguageÂ Models.

Table 5.Â Summary of Friedman test results for different metrics and promptingÂ conditions.

![image](https://github.com/user-attachments/assets/1298c28e-e47b-46ad-bac4-7e928b780ae7)

The F1 score for named entity recognition (NER) reveals substantial model variability, both under single-task and multitask conditions. TheÂ Friedman statistic for single-task prompting is 1030.82, withÂ a correspondingÂ p-value ofÂ 7.49Ã—10âˆ’2227.49Ã—10âˆ’222. This already indicates a strong differentiation among the models. InÂ the multitask setup, theÂ statistic slightly increases to 1050.92, withÂ an even lowerÂ p-value ofÂ 3.29Ã—10âˆ’2263.29Ã—10âˆ’226, reinforcing that the multitask strategy introduces subtle, yet statistically detectable, shifts in modelÂ performance.The BLEU score, used for evaluating the quality of text generation in review scenarios, shows a notable significance across the models. InÂ the single-task scenario, theÂ Friedman statistic is 1002.16, withÂ aÂ p-value ofÂ 1.21Ã—10âˆ’2151.21Ã—10âˆ’215, suggesting strong differences in text generation abilities when the models are prompted individually. TheÂ multitask configuration amplifies this distinction dramatically, withÂ the statistic reaching 1970.88 and aÂ p-value effectively equal to 0, indicating extremely robust model variability under multitaskÂ prompts.Sentiment classification, measured by the Exact-Match score, also displays clear differences, althoughÂ the statistics are relatively lower compared to F1 and BLEU scores. TheÂ Friedman test for single-task conditions yields a statistic of 655.21 and aÂ p-value ofÂ 1.73Ã—10âˆ’1401.73Ã—10âˆ’140, which is still well within the range of significance. UnderÂ multitask conditions, theÂ statistic increases to 792.74, withÂ a correspondingÂ p-value ofÂ 2.87Ã—10âˆ’1702.87Ã—10âˆ’170, suggesting that multitask prompting potentially offers a more nuanced differentiation between the models, even in simpler binary classificationÂ tasks.The overall Score metric, synthesizing various aspects of model performance, presents the most pronounced differences. TheÂ Friedman statistic under single-task conditions is 1310.58, withÂ an extremely lowÂ p-value ofÂ 1.69Ã—10âˆ’2821.69Ã—10âˆ’282, indicating that single-task prompts already cause marked differences in overall performance. This differentiation becomes even more pronounced in the multitask scenario, where the statistic soars to 1808.26 with aÂ p-value of 0, highlighting a profound impact of multitask prompts on overall modelÂ effectiveness.
The statistical evidence gathered from this study confirms that different prompting strategies have a tangible impact on model behavior across various evaluation metrics. This underscores the importance of carefully selecting prompting techniques based on the desired outcome. While single-task prompts provide a baseline for evaluating individual task performance, multitask prompting introduces additional complexity that can either enhance or differentiate model outputs more significantly, depending on the metric and task at hand. Future research should explore why certain models react more sensitively to prompt changes, potentially investigating architectural factors or training data characteristics that influence prompt responsiveness. Additionally, expanding the evaluation to include more diverse tasks and model types would offer further insights into the generalizability of theseÂ findings.

### 4.2. Descriptive Statistics Related to ModelÂ Performance

### 4.2.1. Gemma 2Â 9B
Gemma 2 9B outperformed all other models in this study. AlthoughÂ the assumption that single-task prompts yield better results compared to the dual, multitask approach still holds true, it is worth noting that the difference in performance is not particularly pronounced. Shifting the focus to individual tasks, theÂ table below provides a detailed breakdown of theÂ results.The data presented in theÂ Table 6Â highlight the comparative performance of the multitask and single-task approaches for the Gemma 2 9B model across various metrics. TheÂ single-task approach demonstrates a slight edge in certain areas, such as Exact-Match accuracy on sentiment analysis, achieving 91.50% compared to 90.00% for the multitask approach, andÂ in the NER F1 score, withÂ a small improvement from 54.75% to 55.75%.

Table 6.Â Specific-task performances on Gemma 2Â 9B.

![image](https://github.com/user-attachments/assets/0ebec001-415f-409c-bd23-81d81f6f0ac4)

However, theÂ multitask strategy shows advantages in NER Precision, scoring 60.99% versus 59.86% for the single-task prompts, suggesting that the multitask model may be more accurate in identifying named entities, thoughÂ it comes at the cost of a slightly lower NER Recall (54.11% for multitask versus 56.87% for single-task).Additionally, theÂ formatting error rate is similar across both approaches, withÂ a marginal difference of 1 per thousand (9.00â€°for multitask versus 8.00â€°for single-task), indicating that the complexity of the prompt does not significantly impact the modelâ€™s ability to maintain proper formatting. These data suggest that while single-task prompts may offer a slight performance benefit in certain aspects, theÂ differences are generally minor, andÂ multitask prompting retains certain advantages, particularly inÂ precision.

### 4.2.2. Qwen 2Â 7B

For Qwen 2 7B, theÂ assumption that single-task prompts yield better results compared to the dual, multitask approach remains valid. However, inÂ contrast to Gemma 2, theÂ observed pattern is more erratic, andÂ the performance gap between the two approaches becomes more pronounced in thisÂ case, as shown inÂ Table 7.

Table 7.Â Specific-task performances on Qwen 2Â 7B.

![image](https://github.com/user-attachments/assets/09748af3-bd10-48bd-abd0-18614c941266)

Table 7Â highlights a more significant divergence between the multitask and single-task approaches for Qwen 2 7B compared to Gemma 2 9B. Single-task prompts outperform multitask prompts across most metrics, particularly in the Mean BLEU score on review tasks (73.26% vs. 56.09%), indicating a substantial advantage in generating coherent and accurate text for single-task prompts. Similarly, theÂ Exact-Match score for sentiment analysis is higher for the single-task approach (82.70% vs. 80.80%).However, anÂ interesting deviation can be observed in NER Precision, where multitask prompts demonstrate better performance (32.20% vs. 27.97%), suggesting that Qwen 2 7Bâ€™s ability to precisely recognize named entities benefits from the complexity of the multitask setup. Despite this, single-task prompts yield higher NER Recall (30.79% vs. 24.32%), reflecting a trade-off between precision and recall similar to what was observed in the previous model. Additionally, theÂ formatting error rate is notably higher for single-task prompts (88.00â€°vs. 34.00â€°), suggesting that while single-task prompts may improve content accuracy, they introduce a greater risk of formatting errors, aÂ factor worth considering in practicalÂ applications.

### 4.2.3. LLama 3.1Â 8B

LLama 3.1 8B is the first model to deviate from the expected pattern. InÂ contrast to previous models, theÂ multitask approach outperforms the single-task approach, demonstrating a clear reversal of the trends observedÂ earlier. LLama 3.1 8B performances are described inÂ Table 8.
Table 8.Â Specific-task performances on LLama 3.1Â 8B.

![image](https://github.com/user-attachments/assets/edae2f59-7191-490c-a1e8-936ee0a50dfe)

Table 8Â reveals a distinctive performance pattern for LLama 3.1 8B, where the multitask approach shows superior results compared to the single-task approach across most metrics. Notably, theÂ Mean BLEU score on review tasks is significantly higher for multitask prompts (88.94% vs. 76.55%), indicating that LLama 3.1 8B generates more coherent and contextually accurate responses when handling multiple tasks simultaneously. Similarly, inÂ sentiment analysis, theÂ multitask approach outperforms the single-task approach with a higher Exact-Match score (83.70% vs. 81.00%). However, theÂ NER metrics present a more nuanced picture. While the single-task approach achieves a slightly higher F1 score (44.10% vs. 43.00%) and NER Recall (46.01% vs. 42.05%), multitask prompts excel in NER Precision (50.25% vs. 47.98%). This suggests that LLama 3.1 8B is more precise but slightly less comprehensive in recognizing named entities when dealing with multitask prompts. Additionally, theÂ formatting error rate is notably lower in the multitask setting (69.00â€°vs. 94.00â€°), indicating that multitask prompts not only yield better content accuracy but also lead to fewer formatting errors. These results underscore the modelâ€™s capacity to handle multitask scenarios effectively, challenging the conventional assumption that single-task prompting is inherentlyÂ superior.

### 4.2.4. Phi 3Â Medium

Regarding Phi3 Medium 14B, based on its performances listed inÂ Table 9, it can be unequivocally stated that its performance was the worst among all models in the experimental set. TheÂ difference in performance between the two prompting approaches is particularly stark, withÂ the single-task approach significantly outperforming the multitaskÂ approach.
Table 9.Â Specific-task performances on Phi 3Â Medium.

![image](https://github.com/user-attachments/assets/a94628df-6f49-4578-8637-dc724e7b6f2c)

Table 9Â illustrates that Phi3 Medium 14B exhibits the weakest overall performance across all evaluated models. TheÂ results clearly demonstrate a substantial gap between the multitask and single-task approaches, withÂ the latter consistently outperforming the former. ForÂ instance, theÂ Mean BLEU score on review tasks is notably higher for single-task prompts (57.63% vs. 16.98%), indicating that Phi3 Medium 14B struggles significantly with generating coherent text in multitask scenarios. Similarly, theÂ Exact-Match score for sentiment analysis shows a slight but consistent improvement in single-task settings (50.80% vs. 48.30%).
The disparity is even more pronounced in NER tasks, where the single-task approach nearly doubles the F1 score (22.62% vs. 11.68%) and achieves higher Precision (25.45% vs. 14.49%) and Recall (23.78% vs. 11.06%). These results suggest that Phi3 Medium 14Bâ€™s ability to recognize and categorize named entities is severely hindered in multitaskÂ settings.
Moreover, both approaches exhibit high formatting error rates, withÂ the single-task method slightly worse (307.00â€°vs. 253.00â€°). This suggests that, althoughÂ the single-task approach improves performance in content-related tasks, both prompting methods struggle with formatting precision. These results position Phi3 Medium 14B as the least capable model in handling complex or multitask scenarios, emphasizing the limitations of this particular architecture in the context of large-scale languageÂ models.

### 4.2.5. MistralÂ 7B

The followingÂ Table 10Â reflects the mixed performance of Mistral 7B across various tasks, showcasing both strengths and weaknesses depending on the task and prompting approach.
Table 10.Â Specific-task performances on MistralÂ 7B.

![image](https://github.com/user-attachments/assets/221f6775-b920-4eca-8d86-eb025b25a453)

In terms of review generation, theÂ single-task approach achieves a higher Mean BLEU score (76.41% vs. 70.84%), indicating better text generation performance for single-task prompts. However, forÂ sentiment analysis, theÂ multitask approach far outperforms the single-task method with a notably higher Exact-Match score (87.20% vs. 71.80%). In named entity recognition (NER), theÂ results are more nuanced. While the single-task approach yields a slightly higher F1 score (32.56% vs. 30.60%) and Recall (33.50% vs. 28.04%), theÂ multitask approach achieves better Precision (39.28% vs. 37.05%), highlighting a trade-off between completeness and accuracy in entity recognition. One of the most significant differences is observed in the formatting error rate, where the multitask approach significantly outperforms the single-task approach, withÂ a much lower error rate (29.00â€°vs. 155.00â€°). This suggests that multitask prompts not only excel in specific tasks, such as sentiment analysis, butÂ also lead to more reliable formatting outputs. TheÂ overall distribution of scores confirms that Mistral 7B performs better under multitask settings in certain scenarios, althoughÂ single-task prompts still offer advantages in specific metrics like BLEU and NERÂ Recall.
The experiments conducted on the five modelsâ€”Gemma 2 9B, Qwen 2 7B, LLama 3.1 8B, Phi3 Medium 14B, andÂ Mistral 7Bâ€”present a diverse and complex landscape of performance across single-task and multitask prompting approaches. While the general assumption that single-task prompts yield superior results holds true for most models, theÂ nuances of performance reveal significant variations depending on the architecture and task type. Gemma 2 9B and Qwen 2 7B conform to expectations, withÂ single-task prompts outperforming multitask prompts across the majority of metrics, thoughÂ the difference is more pronounced in Qwen 2 7B. InÂ contrast, LLama 3.1 8B challenges this assumption, demonstrating superior results with multitask prompts, particularly in generating coherent text and sentiment analysis, signaling that not all models adhere to a uniform performance pattern. Phi3 Medium 14B exhibited the weakest overall performance, withÂ both single-task and multitask approaches underperforming compared to the other models, butÂ with the single-task approach consistently outperforming the multitask one. This highlights potential limitations in the modelâ€™s architecture when handling both simple and complex tasks. Mistral 7B presents a mixed profile, withÂ performance fluctuating between the two prompting approaches depending on the task. While single-task prompts show an advantage in text generation and NER Recall, multitask prompts excel in sentiment analysis and NER Precision, withÂ a notably lower formatting error rate, suggesting that Mistral 7B is more versatile but less predictable. Overall, these experiments underscore the importance of selecting the appropriate prompting strategy based on the specific model and task at hand. While single-task prompts generally offer better performance, certain models like LLama 3.1 8B and Mistral 7B demonstrate the potential of multitask prompts to exceed single-task results in specific contexts. TheÂ diverse outcomes across these models suggest that optimizing prompting strategies for Large Language Models should be model-specific and task-aware, rather than guided by a one-size-fits-allÂ approach.

### 5. Discussion

This study provides an in-depth comparison of single-task and multitask prompting strategies for Large Language Models (LLMs) across five models: Gemma 2 9B, Qwen 2 7B, LLama 3.1 8B, Phi3 Medium 14B, andÂ Mistral 7B. Contrary to the initial hypothesis, theÂ results indicate that single-task prompts do not consistently outperform multitask prompts across all scenarios. TheÂ analysis revealed that, while single-task prompts often excel in structured tasks like sentiment analysis and generate higher BLEU scores for review generation, multitask prompts have their own advantages. These include higher precision in named entity recognition (NER) tasks for certain models and a reduced frequency of formatting errors, highlighting the potential of multitask prompts for integrated tasks. This variability underscores the significant impact of model architecture on the effectiveness of promptÂ strategies.The findings demonstrate that prompt strategy effectiveness is model-dependent. ForÂ instance, Gemma 2 9B performed well with both prompt strategies, showing only minor differences between single-task and multitask approaches. This suggests that more capable models, like Gemma 2 9B, are relatively unaffected by prompt complexity, maintaining stable performance regardless of prompt type. However, forÂ models like Qwen 2 7B and Phi3 Medium 14B, single-task prompts clearly outperformed multitask prompts, particularly in review generation, where BLEU scores were notably higher with single-task prompts. This suggests that models with lower capacity or different architectures might struggle with the complexity introduced by multitask prompts; on the other hand, models such as LLama 3.1 8B and Mistral 7B showed unexpected strengths with multitask prompts. LLama 3.1 8B, inÂ particular, achieved higher BLEU scores and exhibited better sentiment classification accuracy in multitask scenarios compared to single-task ones, suggesting that its architecture might be better suited to handling integrated prompts that require simultaneous consideration of multiple tasks.This is an indication that multitask prompts might enhance contextual understanding in models designed to manage complex dependencies within the text.Different tasks yielded varying results depending on the prompting strategy. InÂ sentiment analysis, single-task prompts generally offered better performance across most models, leading to higher accuracy scores. This supports the notion that simpler and more focused prompts can enhance performance in tasks that require clear-cut, binary decisions. However, inÂ NER, multitask prompts occasionally demonstrated higher precision, particularly in LLama 3.1 8B and Mistral 7B, suggesting that integrated prompts may help in extracting contextual information relevant to entity recognition. Additionally, theÂ multitask prompt strategy resulted in a lower error rate for JSON formatting across several models, indicating that multitask prompts might provide advantages in tasks requiring consistent output structures. This insight is critical when choosing between single-task and multitask approaches, asÂ the task requirements and desired outcomes must be carefully matched to the strengths of each promptingÂ strategy.The studyâ€™s findings challenge the common assumption that single-task prompts are universally superior due to their simplicity. While they frequently lead to higher precision in certain contexts, multitask prompts have demonstrated advantages in integrating multiple outputs and maintaining structural consistency. This suggests that prompt engineering must be flexible, adapting to both the taskâ€™s complexity and the specific architecture of the LLM. Moreover, theÂ results showed no straightforward correlation between model size and prompt effectiveness. Larger models like Phi3 Medium did not always benefit from multitask prompts, while smaller ones like LLama 3.1 8B performed unexpectedly well with integrated tasks. These observations suggest that elements beyond size, such as attention mechanisms and the training datasetâ€™s nature, significantly influence how LLMs respond to various promptÂ strategies.This studyâ€™s scope is constrained by several factors. TheÂ selected models, while diverse, represent a subset of open-weight LLMs, limiting the generalizability of findings to proprietary or cutting-edge models. Additionally, theÂ focus was on a narrow range of NLP tasksâ€”sentiment analysis, NER, andÂ JSON formattingâ€”leaving room for further exploration in more diverse and challenging scenarios like machine translation or complex reasoning tasks. Future research should aim to broaden the scope by incorporating a wider variety of tasks and models, potentially examining how specific architectural elements, such as attention head count or layer depth, influence prompt performance. It would also be valuable to investigate automated prompt optimization methods, such as reinforcement learning, toÂ dynamically adapt prompts to maximize model efficiency. Exploring domain-specific challenges, like legal or medical texts, could provide deeper insights into the applicability of single-task versus multitask strategies in specializedÂ contexts.
From a practical standpoint, theÂ studyâ€™s findings provide actionable guidance for developers aiming to optimize LLM interactions. TheÂ data underscore the need for a context-sensitive approach to prompt engineering, where prompt choice is not driven by simplicity alone but by a careful assessment of the taskâ€™s requirements and the modelâ€™s characteristics. Theoretically, this study challenges the dominance of single-task prompts in the field, revealing that effective prompt strategies are highly dependent on the LLMâ€™s architecture and the nature of the task. These findings advocate for a more nuanced approach to prompt design, moving away from one-size-fits-all solutions and towards strategies tailored to the specificÂ context.Statistical tests corroborated the complexity of LLM responses to different prompt types. ForÂ example, non-parametric tests like the Wilcoxon signed-rank test highlighted significant differences in BLEU and NER F1 scores between single-task and multitask prompts. Such results point to deeper, architecture-specific factors influencing prompt effectiveness, suggesting that the interplay between task complexity and model design is more intricate than previously thought. To address this, future research must delve deeper into understanding how LLM architectures process prompts. Greater clarity on the internal workings of LLMs, particularly in how they handle varied prompt structures, will be crucial for developing more reliable AI systems. This is especially relevant in high-stakes domains like healthcare or finance, where transparency and consistency in AI behavior areÂ paramount.

### 6. Conclusions

This comparative study on the effectiveness of multitask versus single-task prompts for Large Language Models (LLMs) has produced nuanced and unexpected findings, challenging the initial assumption that single-task prompts would consistently outperform multitaskÂ ones.Among the tested models, Gemma 2 9B exhibited the strongest overall performance, showing only a slight preference for single-task prompts. TheÂ difference between the two prompting strategies was marginal, withÂ nearly equivalent results in sentiment exact match (91.50% for single-task vs. 90.00% for multitask) and F1 score for entity recognition (55.75% vs. 54.75%). Qwen 2 7B demonstrated a clearer advantage for single-task prompts, particularly in text generation, asÂ evidenced by a higher BLEU score (73.26% vs. 56.09%). However, multitask prompts outperformed in NER precision (32.20% vs. 27.97%), revealing a notable trade-off between precision and recall. LLama 3.1 8B defied expectations by favoring multitask prompts, which yielded higher BLEU scores (88.94% vs. 76.55%) and better sentiment exact match (83.70% vs. 81.00%), suggesting that this model handles multitasking with greater consistency and precision in text generation and sentiment analysis. Conversely, Phi3 Medium 14B showed the weakest overall performance, withÂ a distinct preference for single-task prompts. TheÂ disparity was particularly significant in the BLEU score (57.63% for single-task vs. 16.98% for multitask) and in the F1 score for NER (22.62% vs. 11.68%), indicating significant challenges in processing multitask scenarios. Mistral 7B displayed the most variable results. While single-task prompts yielded a better BLEU score (76.41% vs. 70.84%), multitask prompts excelled in sentiment exact match (87.20% vs. 71.80%) and resulted in a notably lower formatting error rate (29.00â€° vs. 155.00â€°), indicating its capacity to handle multitasking effectively in someÂ contexts.These findings highlight the fact that the effectiveness of prompts varies significantly across models. InÂ some cases, such as with LLama 3.1 8B and Mistral 7B, multitask prompts proved to be more effective, challenging the assumption that simplicity in prompting always leads to better outcomes. TheÂ analysis also revealed complex trade-offs, particularly in tasks requiring a balance between precision and recall, asÂ observed in NER. This suggests that the choice between single-task and multitask prompts can significantly impact not only overall accuracy but also how well a model handles the nuanced aspects of performance. The practical implications of this study are substantial. It provides concrete guidance on selecting effective prompts for different LLM architectures and applications, emphasizing the need for a nuanced, model-specific approach to prompt design. Furthermore, theÂ results underscore the importance of understanding the underlying factors that drive performance variations. Future research should focus on unpacking these mechanisms to gain a deeper understanding of how model architecture, task characteristics, andÂ prompt complexity interact. This would lay the groundwork for optimizing prompts and refining LLM performance in real-worldÂ contexts.
